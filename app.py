import streamlit as st
import torch
from transformers import AutoTokenizer
import os
import sys
from model.microGPT import MicroGPT
from utils.utils import sample_output



# æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
# MODEL_WEIGHTS_PATH = "./assert/micro_gpt_chat.pth"
MODEL_WEIGHTS_PATH = "./assert/micro_gpt_chat_0_ing.pth"
# MODEL_WEIGHTS_PATH = "./assert/micro_gpt_pretrain_1.pth"
# åˆ†è¯å™¨ (Tokenizer) æ‰€åœ¨ç›®å½•
# æ³¨æ„ï¼šè¿™åº”è¯¥ä¸æ‚¨è®­ç»ƒæ—¶ä½¿ç”¨çš„ DATA_DIR ä¸€è‡´
DATA_DIR = "/media/liuzh/data/DLData/minimind/"

# microGPT æ¨¡å‹å‚æ•° (å¿…é¡»ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´)
VOCAB_SIZE = 6400 # å‡è®¾è¿™æ˜¯æ‚¨çš„è¯æ±‡è¡¨å¤§å°ï¼Œå¦‚æœæ¨¡å‹æœªåœ¨è®­ç»ƒæ—¶ä½¿ç”¨ç‰¹æ®Šæ ‡è®°ï¼Œé€šå¸¸æ˜¯è¿™ä¸ªå€¼
D_MODEL = 512
NHEAD = 8
NUM_LAYERS = 12
D_FF = D_MODEL * 4
DROPOUT = 0.0

# è®¾ç½®è®¾å¤‡
# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
DEVICE = torch.device("cpu")




@st.cache_resource
def load_model_and_tokenizer():
    """åœ¨åº”ç”¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    try:
        # 1. åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(DATA_DIR)

        # 2. ç¡®å®š VOCAB_SIZE
        global VOCAB_SIZE
        VOCAB_SIZE = len(tokenizer)
        
        # 3. åˆå§‹åŒ–æ¨¡å‹
        model = MicroGPT(VOCAB_SIZE, D_MODEL, NHEAD, NUM_LAYERS, D_FF, DROPOUT)
        
        # 4. åŠ è½½æƒé‡
        state_dict = torch.load(MODEL_WEIGHTS_PATH, map_location='cpu')
        model.load_state_dict(state_dict)
        
        # 5. è¿ç§»åˆ°è®¾å¤‡å¹¶è®¾ç½®ä¸º bfloat16 (ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´)
        model = model.to(DEVICE, dtype=torch.bfloat16)
        model.eval()

        st.success(f"âœ… æ¨¡å‹ microGPT å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸï¼Œè¿è¡Œåœ¨ {DEVICE} ä¸Šã€‚")
        return model, tokenizer
        
    except Exception as e:
        st.error(f"âŒ æ¨¡å‹æˆ–åˆ†è¯å™¨åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„å’Œæ¨¡å‹å‚æ•°ï¼é”™è¯¯: {e}")
        st.stop()
        
# --- Streamlit ç•Œé¢ä¸»ä½“ ---
def main():
    st.set_page_config(
        page_title="MicroGPT Chatbot",
        page_icon="",
        layout="wide"
    )


    st.title("MicroGPT å¯¹è¯ç•Œé¢")

    # æç¤ºè¯å’Œä¿¡æ¯ç»„ç»‡
    with st.expander("ğŸ“ ç¤ºä¾‹æç¤ºè¯å’Œä½¿ç”¨è¯´æ˜", expanded=False):
        st.markdown("""
            æ¬¢è¿ä½¿ç”¨ **MicroGPT** æ¨¡å‹èŠå¤©åº”ç”¨ã€‚
            
            - **æ¨¡å‹å‚æ•°:** æ¨¡å‹é…ç½®ä¸º $D_{model}=512$, $N_{layers}=12$ã€‚
            - **ä½¿ç”¨æŠ€å·§:** å°è¯•åœ¨ä¾§è¾¹æ è°ƒæ•´ **æ¸©åº¦** å’Œ **Top-K** å‚æ•°æ¥è§‚å¯Ÿç”Ÿæˆç»“æœçš„å˜åŒ–ã€‚
            
            **æ¨èç¤ºä¾‹:**
            * â€œè§£é‡Šä¸€ä¸‹â€˜å…‰åˆä½œç”¨â€™çš„åŸºæœ¬è¿‡ç¨‹â€
            * â€œè¯·ç”¨ Python å†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°â€
            * â€œå¦‚ä½•æ‰èƒ½æ›´å¥½åœ°å­¦ä¹ æ·±åº¦å­¦ä¹ ï¼Ÿâ€
        """)

    st.info(f"âœ¨ å½“å‰æ¨¡å‹å‚æ•°: **{D_MODEL}** ç»´åº¦, **{NUM_LAYERS}** å±‚ã€‚")

    # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = load_model_and_tokenizer()

    # 2. ä¾§è¾¹æ ï¼šå‚æ•°è®¾ç½®
    with st.sidebar:
        st.header("âš™ï¸ æ¨ç†å‚æ•°è®¾ç½®")
        
        # æ¸©åº¦ (Temperature) æ»‘å—
        temperature = st.slider(
            "æ¸©åº¦ (Temperature)",
            min_value=0.01,
            max_value=1.5,
            value=0.8,
            step=0.01,
            help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚æ¸©åº¦è¶Šé«˜ï¼Œç»“æœè¶Šå¤šæ ·åŒ–ï¼ˆè¶Šéšæœºï¼‰ã€‚"
        )

        # Top-K é‡‡æ ·æ»‘å—
        top_k = st.slider(
            "Top-K",
            min_value=1,
            max_value=100, # æœ€å¤§å€¼ä¸ºè¯æ±‡è¡¨å¤§å°
            value=50,
            step=1,
            help="é™åˆ¶æ¨¡å‹åªä»æ¦‚ç‡æœ€é«˜çš„ K ä¸ªè¯ä¸­é‡‡æ ·ã€‚K è¶Šå°ï¼Œç”Ÿæˆè¶Šä¿å®ˆã€‚"
        )

        # æœ€å¤§ç”Ÿæˆé•¿åº¦
        max_new_tokens = st.slider(
            "æœ€å¤§ç”Ÿæˆé•¿åº¦ (Max New Tokens)",
            min_value=10,
            max_value=512,
            value=256,
            step=10,
            help="æ¨¡å‹å•æ¬¡å›ç­”ç”Ÿæˆçš„æœ€é•¿ Token æ•°ã€‚"
        )
        
        st.info("ğŸ’¡ **æç¤º:** æ›´æ”¹å‚æ•°åï¼Œæ–°ä¸€è½®å¯¹è¯å°†ä½¿ç”¨æ–°å‚æ•°ã€‚")

    # 3. å¯¹è¯å†å²åˆå§‹åŒ–
    if "messages" not in st.session_state:
        # åˆå§‹åŒ–å¯¹è¯å†å²ï¼ŒåŒ…å«ä¸€ä¸ªç³»ç»Ÿæç¤º
        st.session_state.messages = [
            {"role": "system", "content": "You are a helpful assistant"},
            {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ MicroGPTï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ"}
        ]
        
    # 4. å±•ç¤ºå†å²å¯¹è¯
    # è¿‡æ»¤æ‰ "system" è§’è‰²ï¼Œåªå±•ç¤ºç”¨æˆ·å’ŒåŠ©æ‰‹çš„æ¶ˆæ¯
    for message in st.session_state.messages:
        if message["role"] in ["user", "assistant"]:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

    # 5. å¤„ç†æ–°çš„ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„é—®é¢˜..."):
        # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å†å²è®°å½•
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # å±•ç¤ºç”¨æˆ·è¾“å…¥
        with st.chat_message("user"):
            st.markdown(prompt)

        # è·å–æ¨¡å‹å›ç­”
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– MicroGPT æ­£åœ¨æ€è€ƒ..."):
                # æ„é€ ç”¨äºä¼ é€’ç»™ sample_output çš„ **å½“å‰å®Œæ•´å¯¹è¯ä¸Šä¸‹æ–‡**
                # âš ï¸ æ³¨æ„: sample_output é»˜è®¤æ˜¯å•è½®å¯¹è¯ï¼Œå¦‚æœéœ€è¦å¤šè½®ï¼Œ
                # æ‚¨éœ€è¦ä¿®æ”¹ sample_output ä½¿å…¶æ¥æ”¶å¹¶å¤„ç† st.session_state.messages
                # è€Œä¸æ˜¯è‡ªå·±æ„é€  messages åˆ—è¡¨ã€‚
                
                # å½“å‰ç‰ˆæœ¬çš„ sample_output ä»…æ¥å—ä¸€ä¸ª "prompt" å­—ç¬¦ä¸²ã€‚
                # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åªæŠŠç”¨æˆ·æœ€æ–°çš„ prompt ä¼ å…¥ï¼š
                
                # ç¡®ä¿ sample_output å‡½æ•°å¯ä»¥å¤„ç†æ‚¨è®­ç»ƒæ—¶çš„å¯¹è¯æ ¼å¼
                generated_text = sample_output(
                    prompt, 
                    model, 
                    tokenizer, 
                    DEVICE,
                    MAX_NEW_TOKENS=max_new_tokens,
                    TEMPERATURE=temperature,
                    TOP_K=top_k
                )

            # å±•ç¤ºæ¨¡å‹å›ç­”
            st.markdown(generated_text)
            
            # å°†æ¨¡å‹å›ç­”æ·»åŠ åˆ°å†å²è®°å½•
            st.session_state.messages.append({"role": "assistant", "content": generated_text})

if __name__ == "__main__":
    main()