import torch
import torch.nn as nn
import torch.optim as optim
from datasets import load_dataset
from transformers import AutoTokenizer
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

from dataset.dataset_sft import SFTDataset
from model.microGPT import MicroGPT
from utils.utils import sample_output

import warnings
warnings.filterwarnings("ignore")

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"



# ==============================================================
# å®šä¹‰å‚æ•°******************************************************=
# ==============================================================

# æœ€å¤§åºåˆ—é•¿åº¦
MAX_LEN = 513
# è®­ç»ƒæ‰¹æ¬¡å¤§å°
BATCH_SIZE = 32 

DATASET_FILE_NAME = "sft_512.jsonl" 
DATA_DIR = "/media/liuzh/data/DLData/minimind/"  #æ•°æ®é›†è·¯å¾„ï¼Œä¸‹è½½åœ°å€ï¼šhttps://www.modelscope.cn/datasets/gongjy/minimind_dataset/files



#===============================================================
# åŠ è½½è¯æ±‡è¡¨****************************************************=
# ==============================================================
tokenizer = AutoTokenizer.from_pretrained(DATA_DIR)
VOCAB_SIZE = len(tokenizer)
print(f"è¯æ±‡è¡¨å¤§å° (Vocab Size): {VOCAB_SIZE}")



#===============================================================
# åŠ è½½æ•°æ®é›†****************************************************=
# ==============================================================
full_dataset = SFTDataset(DATA_DIR + DATASET_FILE_NAME, tokenizer, max_length=MAX_LEN)
print(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œæ€»æ ·æœ¬æ•°: {len(full_dataset)}")

## ğŸ“¦ åˆ›å»º DataLoader
full_dataloader = DataLoader(
    full_dataset,
    shuffle=True,  
    batch_size=BATCH_SIZE,
    pin_memory=True,  
    num_workers=4, # æé«˜æ•°æ®åŠ è½½é€Ÿåº¦
)
print(f"\nè®­ç»ƒ DataLoader åˆ›å»ºå®Œæˆï¼Œæ€»æ‰¹æ¬¡æ•°é‡: {len(full_dataloader)}")



#===============================================================
# å®šä¹‰æ¨¡å‹******************************************************=
# ==============================================================
D_MODEL = 512
NHEAD = 8
NUM_LAYERS = 12
D_FF = D_MODEL * 4
DROPOUT = 0.0

micro_gpt = MicroGPT(VOCAB_SIZE, D_MODEL, NHEAD, NUM_LAYERS, D_FF, DROPOUT)
micro_gpt.load_state_dict(torch.load("./assert/micro_gpt_chat.pth"))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")
micro_gpt = micro_gpt.to(device, dtype=torch.bfloat16)




#===============================================================
# å®šä¹‰è®­ç»ƒç­–ç•¥******************************************************=
# ==============================================================
EPOCHS = 1  #ä¸€èˆ¬è®­ç»ƒ1è½®ï¼Œæˆ–è€…2-6è½®
LEARNING_RATE = 5e-4 # å­¦ä¹ ç‡

# æ¢¯åº¦ç´¯ç§¯é…ç½®
GA_STEPS = 4  # æ¯å¤šå°‘æ­¥æ›´æ–°ä¸€æ¬¡æ¢¯åº¦ï¼Œç›¸å½“äºBATCH_SIZE *= GA_STEPS
ITER_STEP = 0 # ç”¨äºè·Ÿè¸ªæ€»çš„è¿­ä»£æ¬¡æ•°
# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
loss_fn = nn.CrossEntropyLoss(ignore_index=-100)
optimizer = torch.optim.AdamW(micro_gpt.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.95))



# ğŸ› ï¸ å…³é”®æ”¹è¿› 1: å­¦ä¹ ç‡è°ƒåº¦å™¨
# æ€»çš„ä¼˜åŒ–æ­¥æ•° (è€ƒè™‘æ¢¯åº¦ç´¯ç§¯)
from torch.optim.lr_scheduler import CosineAnnealingLR # å¯¼å…¥è°ƒåº¦å™¨
# å­¦ä¹ ç‡ Warmup æ­¥æ•°
WARMUP_STEPS = 500
TOTAL_TRAIN_STEPS = (len(full_dataloader) * EPOCHS) // GA_STEPS
# Cosine Annealing è°ƒåº¦å™¨ (T_max æ˜¯å‘¨æœŸï¼Œè¿™é‡Œè®¾ä¸ºæ€»æ­¥æ•°)
scheduler = CosineAnnealingLR(optimizer, T_max=TOTAL_TRAIN_STEPS - WARMUP_STEPS, eta_min=1e-6) 
# Warmup åˆå§‹å­¦ä¹ ç‡
WARMUP_START_LR = 1e-7



#===============================================================
# è®­ç»ƒæ¨¡å‹******************************************************=
# ==============================================================
def get_lr_warmup(step, max_lr, start_lr, warmup_steps):
    """è®¡ç®— Warmup é˜¶æ®µçš„å­¦ä¹ ç‡"""
    if step < warmup_steps:
        return start_lr + (max_lr - start_lr) * (step / warmup_steps)
    return max_lr

train_loss_history = []
for epoch in range(EPOCHS):
    micro_gpt.train()
    total_loss = 0
    iter_step = 0

    optimizer.zero_grad()
    for (input_ids, labels, loss_mask) in tqdm(full_dataloader):
        # ========================================================================================#
        # ===============æ„é€ è¾“å…¥è¾“å‡ºæ•°æ®============================================================#
        # ========================================================================================#
        input_ids = input_ids.to(device)
        labels = labels.to(device)
        loss_mask = loss_mask.to(device)

        labels = labels.clone() # åˆ›å»º labels çš„å‰¯æœ¬ï¼Œä¸ä¿®æ”¹dataloaderä¸­çš„åŸå§‹æ•°æ®
        labels[loss_mask == 0] = -100  # å°†maskä½ç½®ç½®ä¸º-100ï¼Œè¡¨ç¤ºå¿½ç•¥è¿™äº›ä½ç½®çš„æŸå¤±è®¡ç®—

        # 1) æ„é€  key_padding_maskï¼šå“ªé‡Œä¸æ˜¯ padï¼Œå°±è®¾ 1
        key_padding_mask = (input_ids == tokenizer.pad_token_id).bool()


        # ğŸ› ï¸ å…³é”®æ”¹è¿› 2: å­¦ä¹ ç‡æ›´æ–°é€»è¾‘
        # 1. Warmup é˜¶æ®µ
        if ITER_STEP < WARMUP_STEPS:
            lr = get_lr_warmup(ITER_STEP, LEARNING_RATE, WARMUP_START_LR, WARMUP_STEPS)
            for param_group in optimizer.param_groups:
                param_group['lr'] = lr
        
        # 2. Cosine Annealing é˜¶æ®µ
        elif ITER_STEP % GA_STEPS == 0:
            # åªæœ‰åœ¨æ¢¯åº¦æ›´æ–°æ—¶æ‰è°ƒç”¨ scheduler.step()
            pass # è°ƒåº¦å™¨å°†åœ¨ optimizer.step() ä¹‹åè°ƒç”¨


        # ========================================================================================#
        # ================è®­ç»ƒæ¨¡å‹ï¼Œè®¡ç®—æŸå¤±=========================================================#
        # ========================================================================================#
        with torch.autocast(device_type="cuda", dtype=torch.bfloat16):  # BF16 autocast, æ··åˆç²¾åº¦è®­ç»ƒ
            outputs = micro_gpt(input_ids, key_padding_mask=key_padding_mask) # [batch, seq, vocab]
            # äº¤å‰ç†µè®¡ç®—æŸå¤±
            loss = loss_fn(outputs.reshape(-1, VOCAB_SIZE), labels.reshape(-1))
            loss = loss / GA_STEPS
        
        # åå‘ä¼ æ’­ï¼ˆç´¯ç§¯æ¢¯åº¦ï¼‰
        loss.backward()
        total_loss += loss.item() * GA_STEPS
        ITER_STEP += 1

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç´¯ç§¯æ­¥æ•°
        if ITER_STEP % GA_STEPS == 0:
            torch.nn.utils.clip_grad_norm_(micro_gpt.parameters(), 1.0) # æ¢¯åº¦è£å‰ª
            optimizer.step()
            optimizer.zero_grad()

            # 3. åœ¨ Cosine é˜¶æ®µæ›´æ–°è°ƒåº¦å™¨
            if ITER_STEP >= WARMUP_STEPS:
                scheduler.step()
            train_loss_history.append(loss.item())
            current_lr = optimizer.param_groups[0]['lr']


        # ========================================================================================#
        # ===============æ£€æŸ¥æ¨¡å‹æ€§èƒ½===============================================================#
        # ========================================================================================#
        iter_step += 1
        if iter_step % 2000 == 0:
            print(f"Epoch {epoch+1}, Iter {iter_step}, Loss: {total_loss/iter_step}")

            prompts = [
                "ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ",
                "ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„",
                "è¯·ç”¨Pythonå†™ä¸€ä¸ªäºŒåˆ†æŸ¥æ‰¾çš„å‡½æ•°",
                'è§£é‡Šä¸€ä¸‹"å…‰åˆä½œç”¨"çš„åŸºæœ¬è¿‡ç¨‹',
            ]

            for prompt in prompts:
                generated_text = sample_output(prompt, micro_gpt, tokenizer, device)
                print(f"æç¤ºè¯: {prompt}")
                print(f"å›ç­”: {generated_text}")
                print("\n")

            torch.save(micro_gpt.state_dict(), f"micro_gpt_chat_{epoch}_ing.pth")
            micro_gpt.train() # é‡æ–°è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼


    # ä¿å­˜æ¨¡å‹
    torch.save(micro_gpt.state_dict(), f"micro_gpt_chat_{epoch}.pth")